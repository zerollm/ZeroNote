æ€»ç»“ä¸€ä¸‹å½“å‰å¼€æºçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLMs)

A list of open-source LLMs that every practitioner should know.

## Pre-trained Models

### OPT (Meta)
- Abstract: Open Pre-trained Transformer Language Models. The OPT 125M--66B models are available in HuggingFace. 
- Reference: https://github.com/facebookresearch/metaseq


### GPT-NeoX (EleutherAI)
- Abstract: GPT-NeoX-20B is a 20B parameter auto-regressive language model trained on the Pile using the GPT-NeoX library (based on DeepSpeed).
- Reference: https://huggingface.co/EleutherAI/gpt-neox-20b

### ğ—™ğ—¹ğ—®ğ—»-ğ—¨ğ—ŸğŸ® (Google)
- Abstract: Flan-UL2 is an encoder-decoder model based on the T5 architecture.
- Reference: https://huggingface.co/google/flan-ul2

### BLOOM & ğ—ºğ—§ğ—¢ (Bigscience)
- Abstract: a family of models capable of following human instructions in dozens of languages zero-shot.
- Reference: https://huggingface.co/bigscience/bloomz

### GLM-130B (Tsinghua)
- Abstract: An Open Bilingual (English & Chinese) Pre-Trained Model, which pre-trained using the algorithm of General Language Model (GLM).
- Reference: https://github.com/THUDM/GLM-130B

### LLaMA (Meta)
- Abstract: LLaMA is an auto-regressive language model, based on the transformer decoder architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.
- Reference: https://github.com/facebookresearch/llama


## Chat/Instruct Style Models

### Alpaca (Stanford)
- Abstract: Alpaca model is fine-tuned from a 7B LLaMA model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.
- Reference: https://github.com/tatsu-lab/stanford_alpaca

### Dolly (Databricks)
- Abstract: Dolly is an instruction-following LLM using GPT-J and fine tuned on Stanford Alpaca. model size: 12B.
- Reference: https://github.com/databrickslabs/dolly

### ğ—šğ—£ğ—§ğŸ°ğ—”ğ—¹ğ—¹
- Abstract: Train an assistant-style LLM with ~800k GPT-3.5-Turbo Generations based on LLaMa.
- Reference: https://github.com/nomic-ai/gpt4all

### Alpaca-LoRA
- Abstract: Alpaca-LoRA reproducing the Stanford Alpaca results using low-rank adaptation (LoRA).
- Reference: https://github.com/tloen/alpaca-lora


## Inference Tools

### llama.cpp
- Abstract: Inference of LLaMA model in pure C/C++.
- Reference: https://github.com/ggerganov/llama.cpp

### Alpaca.cpp
- Abstract: Locally run an Instruction-Tuned Chat-Style LLM
- Reference: https://github.com/antimatter15/alpaca.cpp
